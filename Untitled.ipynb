{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageDraw \n",
    "import os\n",
    "import copy\n",
    "from torchvision import transforms\n",
    "import ast\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5a4c13b9f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './models'\n",
    "model_ = '{}/doodle_model1570758141_19.pt'.format(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module that defines function that trains the model.\n",
    "\"\"\"\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "def fit(dataloader, model, criterion, optimizer, scheduler=None, num_epochs=25):\n",
    "    \"\"\"\n",
    "    This function trains the model passed by the arguments.\n",
    "    Parameters:\n",
    "        dataloader(dict): Dict that keeps train and val data.\n",
    "        model(torchvision.models): Model to train.\n",
    "        criterion(torch.nn.modules.loss): Defined training loss.\n",
    "        optimizer(torch.optim): Optimizer.\n",
    "        device(torch.device): Device representer where Tensors will be allocated.\n",
    "        num_epochs(int): Number of epochs to train the model.\n",
    "    Returns:\n",
    "        model(torchvision.models): Trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for batch in dataloader[phase]:\n",
    "                inputs = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if scheduler is not None:\n",
    "                            scheduler.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == torch.max(labels, 1)[1])\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), '{}/doodle_model{}_{}.pt'.format(model_path, int(time.time()), epoch))\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def predict(dataset, model, topk=2):\n",
    "    \"\"\"\n",
    "    This function predicts the labels for given dataset.\n",
    "    Parameters:\n",
    "        dataset(torch.utils.data.Dataloader): Dataloader that keeps the data to predict on.\n",
    "        model(torchvision.models): Model to make predictions with.\n",
    "        top_k(int): Top_k predictions to make.\n",
    "    Returns:\n",
    "        (dict): Dictionary with cofidence score, one_hot prediction vector and top_k predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    one_hot = []\n",
    "    confidence = []\n",
    "    top_k = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            images = data['image'].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            top_k.extend(torch.topk(outputs.data, topk).indices.cpu().tolist())\n",
    "            one_hot.extend(torch.nn.functional.one_hot(predicted).cpu().tolist())\n",
    "            confidence.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().tolist())\n",
    "    return {'confidence': confidence, 'one_hot': one_hot, 'top_k': top_k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_topk(output, target, topk):\n",
    "    \"\"\"\n",
    "    Computes the precision@k for the specified values of k\n",
    "    Parameters:\n",
    "        output(Tensor): Output of the model.\n",
    "        target(Tensor): True label.\n",
    "        topk(tuple): Size of k-argument.\n",
    "    Returns:\n",
    "        res(Tensor): Accuracy of top-k.\n",
    "    \"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def accuracy(dataloader, model, topk=2):\n",
    "    \"\"\"\n",
    "    Computes accuracy and top-k of the model.\n",
    "    Parameters:\n",
    "        dataloader(dict): Processed dataset.\n",
    "        model(torchvision.models): Model to evaluate.\n",
    "        topk(int): Size of k-argument.\n",
    "    Returns:\n",
    "        res(Tensor): Accuracy of top-k.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    top_k_acc = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader['test']:\n",
    "            images = data['image'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            top_k_acc += (accuracy_topk(outputs, torch.max(labels, 1)[1], topk=(topk,)))[0]\n",
    "            total += labels.size(0)\n",
    "            batch_count += 1\n",
    "            correct += torch.sum(predicted == torch.max(labels, 1)[1])\n",
    "\n",
    "    print('Accuracy of the network: {:.0f} %'.format(100 * correct / total))\n",
    "    print('-' * 10)\n",
    "    print('Top{} accuracy of the network: {:.0f} %'.format(\n",
    "        topk, top_k_acc.cpu().numpy()[0] / batch_count))\n",
    "\n",
    "def confusion_matrix(dataloader, model, labels):\n",
    "    \"\"\"\n",
    "    Computes f1 score, per-class accuracy and confusion_matrix.\n",
    "    Parameters:\n",
    "        dataloader(dict): Processed dataset.\n",
    "        model(torchvision.models): Model to evaluate.\n",
    "        labels(list): List of classes names.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    nb_classes = len(labels)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    prediction_list = []\n",
    "    labels_list = []\n",
    "    matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader['test']:\n",
    "            images = data['image'].to(device)\n",
    "            label = data['label'].to(device)\n",
    "            labels_list.extend(torch.max(label, 1)[1].cpu().tolist())\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            prediction_list.extend(preds.cpu().tolist())\n",
    "            for true, pred in zip(torch.max(label, 1)[1].view(-1), preds.view(-1)):\n",
    "                matrix[true.long(), pred.long()] += 1\n",
    "\n",
    "    class_acc = (matrix.diag() / matrix.sum(1)).cpu().tolist()\n",
    "\n",
    "    print(\"F1 Weighted score: %.2f\" % f1_score(labels_list, prediction_list, average='weighted'))\n",
    "    print('-' * 10)\n",
    "    print('Per class accuracy:')\n",
    "    print()\n",
    "    for idx, acc in enumerate(class_acc):\n",
    "        print('{0}: {1:.2f} %'.format(labels[idx], acc * 100))\n",
    "    print('-' * 10)\n",
    "\n",
    "\n",
    "    df_cm = pd.DataFrame(matrix.numpy(), labels, labels)\n",
    "    sn.set(font_scale=1.4)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    sn.heatmap(\n",
    "        df_cm,\n",
    "        annot=False,\n",
    "        annot_kws={\"size\": 16},\n",
    "        fmt='g',\n",
    "        square=True,\n",
    "        cbar=False,\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=False,\n",
    "        yticklabels=False\n",
    "    )\n",
    "    plt.savefig('matrix1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(os.listdir('./csv_data')[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoodleDataset(Dataset):\n",
    "    \n",
    "    def draw_it(self, strokes):\n",
    "        image = Image.new(\"P\", (256,256), color=255)\n",
    "        image_draw = ImageDraw.Draw(image)\n",
    "        for stroke in ast.literal_eval(strokes):\n",
    "            for i in range(len(stroke[0])-1):\n",
    "                image_draw.line([stroke[0][i], \n",
    "                                 stroke[1][i],\n",
    "                                 stroke[0][i+1], \n",
    "                                 stroke[1][i+1]],\n",
    "                                fill=0, width=5)\n",
    "        return image.convert('RGB')\n",
    "    \n",
    "    def get_df(self, source_dir, class_list, im_per_class):\n",
    "        li = []\n",
    "        for class_ in class_list:\n",
    "            df = pd.read_csv('{}/{}'.format(source_dir, class_),\n",
    "                             engine='python',\n",
    "                             usecols=['drawing', 'recognized', 'word'],\n",
    "                             nrows=im_per_class*5//4)\n",
    "            df = df[df.recognized == True][['drawing', 'word']]\n",
    "            li.append(df.head(im_per_class))\n",
    "        return pd.concat(li, axis=0, ignore_index=True)\n",
    "    \n",
    "    def __init__(self, source_dir, class_list=[], im_size=224, im_per_class=3000, transform=None):\n",
    "        self.transform = transform\n",
    "        self.im_size = im_size\n",
    "        self.source_dir = source_dir\n",
    "        if len(class_list) < 1:\n",
    "            self.class_list = os.listdir(self.source_dir)\n",
    "        else:\n",
    "            self.class_list = class_list\n",
    "        self.source_df = self.get_df(self.source_dir, self.class_list, im_per_class)\n",
    "        self.one_hot_labels = pd.get_dummies(self.source_df['word'])\n",
    "        \n",
    "    def labels(self):\n",
    "        return self.source_df.word.unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.source_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        class_name = self.source_df.iloc[idx, 1]\n",
    "        label = torch.LongTensor(self.one_hot_labels.iloc[idx].values)\n",
    "        image = self.draw_it(self.source_df.iloc[idx, 0])\n",
    "        sample = {'image': image, 'label': label, 'class_name': class_name}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = DoodleDataset('./csv_data', class_list=os.listdir('./csv_data')[:50], im_per_class=5000)\n",
    "\n",
    "# Splitting the data \n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "test_size = int(0.2 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size - test_size\n",
    "train_dataset, test_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, test_size, val_size]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.255]\n",
    ")\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dataset.transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "test_dataset.dataset.transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "val_dataset.dataset.transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "dataloader = {\n",
    "    'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "    'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=False),\n",
    "    'test': DataLoader(test_dataset, batch_size=batch_size, shuffle=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    sample = train_dataset[i]\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.axis('off')\n",
    "    # Here I am using inverse normalization to display augmented images.\n",
    "    plt.imshow(transforms.ToPILImage()(inv_normalize(sample['image'])))\n",
    "    plt.title(sample['class_name'])\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = torchvision.models.resnet50(pretrained=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "model.load_state_dict(torch.load(model_))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Weighted score: 0.82\n",
      "----------\n",
      "Per class accuracy:\n",
      "\n",
      "bridge: 93.99 %\n",
      "hamburger: 85.04 %\n",
      "bed: 76.95 %\n",
      "bush: 55.32 %\n",
      "paint can: 74.39 %\n",
      "line: 88.97 %\n",
      "teapot: 80.58 %\n",
      "garden: 81.24 %\n",
      "tiger: 87.72 %\n",
      "postcard: 78.79 %\n",
      "cannon: 90.57 %\n",
      "picture frame: 79.51 %\n",
      "tennis racquet: 75.99 %\n",
      "mailbox: 81.77 %\n",
      "sleeping bag: 92.93 %\n",
      "blueberry: 88.14 %\n",
      "campfire: 75.31 %\n",
      "nose: 88.94 %\n",
      "ice cream: 79.53 %\n",
      "floor lamp: 90.53 %\n",
      "lantern: 85.09 %\n",
      "bear: 58.85 %\n",
      "oven: 94.34 %\n",
      "grass: 85.07 %\n",
      "yoga: 59.02 %\n",
      "mushroom: 97.12 %\n",
      "The Eiffel Tower: 66.32 %\n",
      "octopus: 80.26 %\n",
      "streetlight: 91.27 %\n",
      "carrot: 88.02 %\n",
      "pickup truck: 81.03 %\n",
      "shorts: 75.60 %\n",
      "dumbbell: 82.62 %\n",
      "flower: 79.59 %\n",
      "bathtub: 87.79 %\n",
      "eye: 85.11 %\n",
      "hourglass: 85.82 %\n",
      "skull: 87.83 %\n",
      "whale: 85.85 %\n",
      "pig: 82.83 %\n",
      "hot tub: 91.67 %\n",
      "purse: 89.54 %\n",
      "backpack: 77.56 %\n",
      "peas: 85.69 %\n",
      "mug: 85.77 %\n",
      "castle: 98.09 %\n",
      "screwdriver: 75.73 %\n",
      "clarinet: 77.68 %\n",
      "dragon: 91.58 %\n",
      "wine glass: 61.38 %\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix(dataloader, model, full_dataset.labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 82 %\n",
      "----------\n",
      "Top1 accuracy of the network: 82 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(dataloader, model, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading the Resnet18 with pretrained weights and freezing the models layers.\n",
    "\n",
    "if model_:\n",
    "    model = torchvision.models.resnet50(pretrained=False)\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(model_))\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    model = torchvision.models.resnet50(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Adding FC layer to train classifier. Required_grad is true by default here.\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "# Setting loss and optimizer. I chose Cross Entropy Loss because it is moustly used loss function when\n",
    "# training for multiclass classification. Adam is also just state of the art optimization algorithm.\n",
    "# It has its downsides but in my opinion still a better choice here than a SGD.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n",
      "train Loss: 0.4043 Acc: 0.8810\n",
      "val Loss: 0.4356 Acc: 0.8718\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model = fit(dataloader, model, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './doodle.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
