{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageDraw \n",
    "import os\n",
    "import copy\n",
    "from torchvision import transforms\n",
    "import ast\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5a4c13b9f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './models'\n",
    "model_ = '{}/doodle_model1570804817_4.pt'.format(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module that defines function that trains the model.\n",
    "\"\"\"\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "def fit(dataloader, model, criterion, optimizer, scheduler=None, num_epochs=25):\n",
    "    \"\"\"\n",
    "    This function trains the model passed by the arguments.\n",
    "    Parameters:\n",
    "        dataloader(dict): Dict that keeps train and val data.\n",
    "        model(torchvision.models): Model to train.\n",
    "        criterion(torch.nn.modules.loss): Defined training loss.\n",
    "        optimizer(torch.optim): Optimizer.\n",
    "        device(torch.device): Device representer where Tensors will be allocated.\n",
    "        num_epochs(int): Number of epochs to train the model.\n",
    "    Returns:\n",
    "        model(torchvision.models): Trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for batch in dataloader[phase]:\n",
    "                inputs = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if scheduler is not None:\n",
    "                            scheduler.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == torch.max(labels, 1)[1])\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), '{}/doodle_model{}_{}.pt'.format(model_path, int(time.time()), epoch))\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "def predict(dataset, model, topk=2):\n",
    "    \"\"\"\n",
    "    This function predicts the labels for given dataset.\n",
    "    Parameters:\n",
    "        dataset(torch.utils.data.Dataloader): Dataloader that keeps the data to predict on.\n",
    "        model(torchvision.models): Model to make predictions with.\n",
    "        top_k(int): Top_k predictions to make.\n",
    "    Returns:\n",
    "        (dict): Dictionary with cofidence score, one_hot prediction vector and top_k predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    one_hot = []\n",
    "    confidence = []\n",
    "    top_k = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            images = data['image'].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            top_k.extend(torch.topk(outputs.data, topk).indices.cpu().tolist())\n",
    "            one_hot.extend(torch.nn.functional.one_hot(predicted).cpu().tolist())\n",
    "            confidence.extend(torch.nn.functional.softmax(outputs, dim=1).cpu().tolist())\n",
    "    return {'confidence': confidence, 'one_hot': one_hot, 'top_k': top_k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_topk(output, target, topk):\n",
    "    \"\"\"\n",
    "    Computes the precision@k for the specified values of k\n",
    "    Parameters:\n",
    "        output(Tensor): Output of the model.\n",
    "        target(Tensor): True label.\n",
    "        topk(tuple): Size of k-argument.\n",
    "    Returns:\n",
    "        res(Tensor): Accuracy of top-k.\n",
    "    \"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def accuracy(dataloader, model, topk=2):\n",
    "    \"\"\"\n",
    "    Computes accuracy and top-k of the model.\n",
    "    Parameters:\n",
    "        dataloader(dict): Processed dataset.\n",
    "        model(torchvision.models): Model to evaluate.\n",
    "        topk(int): Size of k-argument.\n",
    "    Returns:\n",
    "        res(Tensor): Accuracy of top-k.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    top_k_acc = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader['test']:\n",
    "            images = data['image'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            top_k_acc += (accuracy_topk(outputs, torch.max(labels, 1)[1], topk=(topk,)))[0]\n",
    "            total += labels.size(0)\n",
    "            batch_count += 1\n",
    "            correct += torch.sum(predicted == torch.max(labels, 1)[1])\n",
    "\n",
    "    print('Accuracy of the network: {:.0f} %'.format(100 * correct / total))\n",
    "    print('-' * 10)\n",
    "    print('Top{} accuracy of the network: {:.0f} %'.format(\n",
    "        topk, top_k_acc.cpu().numpy()[0] / batch_count))\n",
    "\n",
    "def confusion_matrix(dataloader, model, labels):\n",
    "    \"\"\"\n",
    "    Computes f1 score, per-class accuracy and confusion_matrix.\n",
    "    Parameters:\n",
    "        dataloader(dict): Processed dataset.\n",
    "        model(torchvision.models): Model to evaluate.\n",
    "        labels(list): List of classes names.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    nb_classes = len(labels)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    prediction_list = []\n",
    "    labels_list = []\n",
    "    matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader['test']:\n",
    "            images = data['image'].to(device)\n",
    "            label = data['label'].to(device)\n",
    "            labels_list.extend(torch.max(label, 1)[1].cpu().tolist())\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            prediction_list.extend(preds.cpu().tolist())\n",
    "            for true, pred in zip(torch.max(label, 1)[1].view(-1), preds.view(-1)):\n",
    "                matrix[true.long(), pred.long()] += 1\n",
    "\n",
    "    class_acc = (matrix.diag() / matrix.sum(1)).cpu().tolist()\n",
    "\n",
    "    print(\"F1 Weighted score: %.2f\" % f1_score(labels_list, prediction_list, average='weighted'))\n",
    "    print('-' * 10)\n",
    "    print('Per class accuracy:')\n",
    "    print()\n",
    "    for idx, acc in enumerate(class_acc):\n",
    "        print('{0}: {1:.2f} %'.format(labels[idx], acc * 100))\n",
    "    print('-' * 10)\n",
    "\n",
    "\n",
    "    df_cm = pd.DataFrame(matrix.numpy(), labels, labels)\n",
    "    sn.set(font_scale=1.4)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    sn.heatmap(\n",
    "        df_cm,\n",
    "        annot=False,\n",
    "        annot_kws={\"size\": 16},\n",
    "        fmt='g',\n",
    "        square=True,\n",
    "        cbar=False,\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=False,\n",
    "        yticklabels=False\n",
    "    )\n",
    "    plt.savefig('matrix1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(os.listdir('./csv_data')[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoodleDataset(Dataset):\n",
    "    \n",
    "    def draw_it(self, strokes):\n",
    "        image = Image.new(\"P\", (256,256), color=255)\n",
    "        image_draw = ImageDraw.Draw(image)\n",
    "        for stroke in ast.literal_eval(strokes):\n",
    "            for i in range(len(stroke[0])-1):\n",
    "                image_draw.line([stroke[0][i], \n",
    "                                 stroke[1][i],\n",
    "                                 stroke[0][i+1], \n",
    "                                 stroke[1][i+1]],\n",
    "                                fill=0, width=5)\n",
    "        return image.convert('RGB')\n",
    "    \n",
    "    def get_df(self, source_dir, class_list, im_per_class):\n",
    "        li = []\n",
    "        for class_ in class_list:\n",
    "            df = pd.read_csv('{}/{}'.format(source_dir, class_),\n",
    "                             engine='python',\n",
    "                             usecols=['drawing', 'recognized', 'word'],\n",
    "                             nrows=im_per_class*5//4)\n",
    "            df = df[df.recognized == True][['drawing', 'word']]\n",
    "            li.append(df.head(im_per_class))\n",
    "        return pd.concat(li, axis=0, ignore_index=True)\n",
    "    \n",
    "    def __init__(self, source_dir, class_list=[], im_size=224, im_per_class=3000, transform=None):\n",
    "        self.transform = transform\n",
    "        self.im_size = im_size\n",
    "        self.source_dir = source_dir\n",
    "        if len(class_list) < 1:\n",
    "            self.class_list = os.listdir(self.source_dir)\n",
    "        else:\n",
    "            self.class_list = class_list\n",
    "        self.source_df = self.get_df(self.source_dir, self.class_list, im_per_class)\n",
    "        self.one_hot_labels = pd.get_dummies(self.source_df['word'])\n",
    "        \n",
    "    def labels(self):\n",
    "        return self.source_df.word.unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.source_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        class_name = self.source_df.iloc[idx, 1]\n",
    "        label = torch.LongTensor(self.one_hot_labels.iloc[idx].values)\n",
    "        image = self.draw_it(self.source_df.iloc[idx, 0])\n",
    "        sample = {'image': image, 'label': label, 'class_name': class_name}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = DoodleDataset('./csv_data', class_list=os.listdir('./csv_data')[:50], im_per_class=5000)\n",
    "\n",
    "# Splitting the data \n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "test_size = int(0.2 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size - test_size\n",
    "train_dataset, test_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, test_size, val_size]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.255]\n",
    ")\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dataset.transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "test_dataset.dataset.transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "val_dataset.dataset.transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "dataloader = {\n",
    "    'train': DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "    'val': DataLoader(val_dataset, batch_size=batch_size, shuffle=False),\n",
    "    'test': DataLoader(test_dataset, batch_size=batch_size, shuffle=False),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    sample = train_dataset[i]\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.axis('off')\n",
    "    # Here I am using inverse normalization to display augmented images.\n",
    "    plt.imshow(transforms.ToPILImage()(inv_normalize(sample['image'])))\n",
    "    plt.title(sample['class_name'])\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = torchvision.models.resnet50(pretrained=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "model.load_state_dict(torch.load(model_))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Weighted score: 0.90\n",
      "----------\n",
      "Per class accuracy:\n",
      "\n",
      "bridge: 96.12 %\n",
      "hamburger: 89.33 %\n",
      "bed: 84.02 %\n",
      "bush: 71.14 %\n",
      "paint can: 85.94 %\n",
      "line: 87.74 %\n",
      "teapot: 87.83 %\n",
      "garden: 88.42 %\n",
      "tiger: 92.94 %\n",
      "postcard: 88.93 %\n",
      "cannon: 94.85 %\n",
      "picture frame: 93.03 %\n",
      "tennis racquet: 83.69 %\n",
      "mailbox: 84.62 %\n",
      "sleeping bag: 91.70 %\n",
      "blueberry: 95.73 %\n",
      "campfire: 85.71 %\n",
      "nose: 93.92 %\n",
      "ice cream: 90.30 %\n",
      "floor lamp: 93.59 %\n",
      "lantern: 95.85 %\n",
      "bear: 75.58 %\n",
      "oven: 95.85 %\n",
      "grass: 96.17 %\n",
      "yoga: 84.53 %\n",
      "mushroom: 96.69 %\n",
      "The Eiffel Tower: 87.94 %\n",
      "octopus: 87.94 %\n",
      "streetlight: 94.43 %\n",
      "carrot: 94.27 %\n",
      "pickup truck: 95.72 %\n",
      "shorts: 79.46 %\n",
      "dumbbell: 86.48 %\n",
      "flower: 91.85 %\n",
      "bathtub: 94.47 %\n",
      "eye: 92.39 %\n",
      "hourglass: 88.04 %\n",
      "skull: 93.42 %\n",
      "whale: 87.18 %\n",
      "pig: 88.93 %\n",
      "hot tub: 94.35 %\n",
      "purse: 96.98 %\n",
      "backpack: 83.46 %\n",
      "peas: 87.05 %\n",
      "mug: 89.33 %\n",
      "castle: 98.40 %\n",
      "screwdriver: 85.24 %\n",
      "clarinet: 89.79 %\n",
      "dragon: 92.38 %\n",
      "wine glass: 80.06 %\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0kAAANJCAYAAADZeXdAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAH8FJREFUeJzt2f+rfdl91/G1zj13ej8TpNwkaC3iL/4F+kuwCFEEiRqqTRHrkNKkCKWldGCIpBAMWAlErJEp2lJQ05A6VtRIS1qLoBhQNBT8E/xBVCrMzKUg0+Pcc/fyhzj6ur2/zPq8952z7/k8Hj/vtc/a39Y5z7P7GGM0AAAAWmut7U49AQAAgC0RSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAAhP3zDHr2R3+y9KE3v/X3SuP5jjFq43t/2p+/BdVz0Np5nAdgPdZWgPvW+L317HJue2+SAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACD0McaYHXQ41j70+vtfL42/+bVXaxPYgPmz/lDv9X0ArKW6rm1hTbtbagdxsdvAQWzAUrwZdlu4GZ646jVo7Tyuw6nXpeqa0pp1ZS1X+7ntvUkCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAQh9jjNlBh+NjTOX9+54f+eXyPn77a58ujV+W6dN2z27XS+OBbZlfSe9763+9Wxr/0d/3Um0CAGeoujZ3P9fOxtV+bntvkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAg9DHGmB10ONY+dP4T7+u9Nr611v7YF/91afx//pk/Uxq/LMWT0Frb7Won4nhXm8P+ovb51fugtXXuhVN797iUxr+0f/r/dSzFm2F3DjcC/F/VtXE0z9MWfmf8zju3pfHf/fJlfRJP3Dn8Tri9q33Ht9ba5UXte/4cfmfcrfC7+UMvzd0Mpz9qAACADRFJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAACEPsYYs4MOx8eYyvu3zE/5gV3vpfHXP/DzpfE3//InSuNba+32uJTGX+5P28grXMZWvIxnofo8VJ8F1uE6rrMmVJ3BaSxblvqF6MUTWb0O1XtpC/eBNWEd1ft5t3Met2CN74dnl3Pbe5MEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAKGPMcbsoMOx9qHzn3hf77XxrbW2FCexK07iIz/01dL41lp761c+W94HT1/1eRqtuINWfx62sCbAVpz6+2kL/vftUhr/XZen/w/YurYNrkNd9Ry2Vj+Pa8zh2eXc9qdfRQAAgLP0VENTJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABA6GOMMTvocHyMqTwtd8v0abvnYtfLc7j+8z9bGn/z658rz+FFt8w/Pg/seu1eqE6h+PGbsMJlKF/L6nU8h+tQtcZ1dB63wbrEVrgX11lbK7ZyDq/2c9t7kwQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAoY8xxuygw/ExpvLBmj/q+3qvjV+qE2it7YqTuP6Bny+Nf/sbP1EaXz2H5+Juqd0LFzsncimew9Za68Ub0v1Ma/XvltbcS62d/jsa1mJN2I6r/dz23iQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQOhjjDE76HB8jKnwQVvmL/09H/nk3ynP4ebXP1fex1NXvQ673leayekUT0Fb4xQsS/E67J7+dQCALar+TmittWeXc9t7k8TJCCQAALZIJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABA6GOMMTvocHyMqfCiuf5L/6C8j5t/9ldXmMnzW+Yfnwd2vZfGV6dQ/HggrLAkeCbbCmtrcfhuV78I1uZtWJbahajeC+fwO6FqK/fy1X5ue2+SAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACD0McaYHXQ4PsZUeGrm75z7eq/P4fqPv1Yaf/Mfv1Iav1RPQmttt8aJgBUsS+1+3u3cy3BOrAmck6v93PbeJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABA6GOMMTvocHyMqTwty/xpu2fXe3kOxSm0Fabw5F1/6hdK42++8eMrzeT5uQ/q56C18zgPp7aFe3ELcwDOhzVlG9b4nn92Obe9N0kAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABA6GOMMTvondvpIffsei+Nn5/xQ8UpbEL1PJz6HKxxHauq5+APfuYfl+fw37/6SnkfFdXnsbXT34vLCjdTb7VJnPp5OgdrXMc17mdO726p3QsXO/fBGo53teuwvzj9daiuK74btmGN34zPLue29yYJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAEIfY4zZQYfjY0zlg3W3TB/2PRe7vtJMTmf+yt9XPYf7i6d/DtfwPT/yy6Xxv/21T680k6erei+31lp3O3ImluLavNvA91v1md7C83wOx0Bd9Xlsrf5MHu9qc6jei2v8Zl5W+KJ/+XJuHt4kAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEDoY4wxO+hwfIypPC23d0tp/OVFvU/nr9x9vZenULIsxQNore12pz2I6jVorX4drv/Cz5XG3/zqT9UmALAxT/37EVjf1X5ue2+SAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgNDHGGN20OH4GFN5scyf9Yd6r+8Drj/x5fI+bn7zp1eYCS866yLvqd4L7gNYz7k8j1f7ue29SQIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACA0McYY3bQ4fgYUwGeqj/ww18vjf+fX//h0vhlfhl7YNd7eR8vuruldh0udvVrsBTnsCvOYYVbscytDOu5vVtK4y8vvI+4PdbOYWv1tbm11j700tw+XDkAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACH2MMWYHHY6PMZUP1jJ/2Pfsel9pJqdTPAVla5zC6jFs4TKewzFUXf+pL5bGv/Vv/0Z5DufwTAOwLt/R27DGb9Znl3Pbe5MEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAKGPMcbsoMPxMabCrPkr93vGt9oOdr3XJrAB1XO4xinYwhxOrXoOPvxnv1yew5u/8fnS+Itd7UJUz0Fr53EvcHruRTgv5/A7Y1lhYXr5cu5AvEkCOANb+BIDgHMhkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAg9DHGmB10OD7GVJi1zF+6e3a9l8YXP74VP34TquegtfM4D6e2LPUL8ZHve600/uY//d3yHF501TWttfq6tgXWVoD1Xe3ntvcmCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAABCH2OM2UGH42NM5f1b5qf8wK73FWYC8B3X3/e50vi3/8PPlsZb0oC0LCv8VtpZWDgfV/u57b1JAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIDQxxhjdtDh+BhTef/+65vvlPfxhz/6cmn8Mn/a7tn1XhoP/H/Fx7G11lr1kazO4Y/85DdqO2it/Ze//6nyPqD6/bYG35F+Z6ylejs7jefxHdtaa88u57b3JgngDAgkALboqYamSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACA0McYY3bQ4fgYUwHgeV3/yb9e3sfNv/ubK8zk+c1/Gz3Ue30fAKxnWeqL+25XX9yv9pOfWf5EAACAMyKSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIfYwxZgcdjo8xlQ/W/FHf1/s686BmKV7InQu5CZ7Hbbj++BdK42++9aWVZgIA67raz23vTRIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAAhD7GGLODDsfahy7zH3nPrvfaBFprxSm0Fabw5J3DOTyHY9iCZSk+0zsnsqp6L7dWv5+/97NvlMb/j6++UpsAm3FXXBMurAll1d9ara3ze+vUqqdhtNP/ZmWd+/nly7lr4U0SAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEPoYY8wOOhwfYyoAvMiuP/mV8j5uvvnaCjM5rflv5ft6X2cegOfxnFzt57b3JgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAQh9jjNlBh2PtQ+c/8b7ea+PXsBQPYreFg4CVLEvxedh5HljH9Se+XBp/85s/XRp/e1xK41tr7XJ/2v8vq9/Ra/AVyVb4fqvbwprSWmvPLue29yYJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAEIfY4zZQYdj7UNv75bS+MsLbQecl2V+Kb5n1/tKM3mxXX/qF0rjb77x4yvN5Gm7PRa/5/e+51lHcWlt1aW1+iy05nlYy9V+bntnHQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAh9jDFmBx2OjzEVXjTzd95Dvdf3QV31WrqO23AO13EpHsSueBDXH3u1NL611m6+/Xp5H3AOqs9za/VnmvNxtZ/b3pskAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAh9jDFmBx2OjzGVF8syf9of2PW+wkygrno7u5Vpzbr4nutPfqU0/uabr600E1501nbOydV+bntvkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAg9DHGmB10OD7GVD5Y80d9X+/rzAOW4s24czOyEdbVbbj+2Kul8Tfffn2lmQCso/pbqbXWXr6c+5LxJgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAQh9jjNlBh+NjTOVpWeZP2z273leaCXAOiktKa621Uy8ry1JcF3fWxS343s++Ud7Hf/uHf6U0fgv3wu+8c1sa/90vX5bG/+67d6XxV5cXpfGt1deU6rp26jWN83K1n9vemySAM+DHBKynGkjA0yeSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACD0McaYHXQ41j50/hPX1/upZ8AWVO9F9xHvOfW6toV78W6pnYSL3ekP4nhXO4b9xemPoWopXsfWWvv9n/5aafybb3ymPIcX3e3dUt7H5cXT/y99KS7Ouy0srqziaj+3/dO/+wEAAFYkkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACH2MMWYHHY6PMZUXy+1xKe/jcq9xT21Zph+fB3a7vsJMXmzzq9hD3WWAzbj+0z9TGn/zb7640kx46qrfD74btmGN7/lnl3Pb+5UNAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAKGPMcbsoMPxMabytMyftft6X2ceT91SPJE7JxKA3+P6k18p7+Pmm6+tMBNOze815+A9V/u57b1J4mSqgQQAj0EgASIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAEIfY4zZQYfjY0wFgKds/tvkvt7XmQc1h9u78j6uLi9WmMnpfPSVXyrv4803PlMa73mCdV3t57b3JgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAQh9jjNlBh+NjTOX9m5/xQ++8WzuID33XvjR+jWPovb6PU1pWOAm7p34SNsC9CLC+6z/3t0vjb37jr600E1501d9bvdW+5LfyG+Fq8qe7N0kAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEPoYY8wOOhwfYyoAz2d+FXuo9/o+4BzcHpfyPi73T/s/2C2sKdcf/0Jp/M23vlSbAJyZq/3c9k97FQMAAFiZSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACA0McYY3bQ4Vj70PlPXF/vp/38ZYWTsDv1QQD/z91Se6arz/MWloOleg52GzgIVvmOPi5Lafzlhf9wqz76yi+V9/HmG58p7+NFt8bztIX1/Rxc7ee2twoBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQOhjjDE76HB8jKm8WObP+kO91/fx1FXPY/Uc3i31C3mxq01iKZ6E3Qo30qmvA+tYY12qWOM+OId78RyOgfPwkR/6amn8W7/y2ZVm8vzOYV176k59Dd7z7HJue2+SAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACD0McaYHXQ4PsZUAHjK5r9N1tf7qWfw9K1xHV2HuqV4IXYbuAjXH/9CafzNt7600kygtav93PbeJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAAChjzHG7KDD8TGm8sGaP+r7ej/t568xBwDWZW1fx6m/o9mG6z/x+fI+bv7931phJpyDq/3c9t4kAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEDoY4wxO+hwrH3o/Ceen95PPYP6dagew7LCjbA78Yl0DN9x6mM4B6d+HteYw3N8ndyz27mPzsXt3VIaf3nhP9zq2txb/XkarfhMb+C74cN/+R+Vxr/9T3+0NH6N37ynPo3n8P3UWmvPLue2twoBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQOhjjDE76HB8jKkwa/7K3df7OvOA27ulNP7yovZ/TfVZaM3zsIYtrEl3S20SF7vaJM7hXjyHY2AdW3imq6rH8OG/+HOl8Te/+lO1CdBaa20pru2ttfbyS3M3pDdJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABD6GGPMDjocax+6zH/kPb312gRaa6PV5rDrtTkUT0FrrbXiFDaheh6cg/M4B7AVd0vtgbzYeSDZBr8ztuH6B3+xvI+bf/FjK8zk+S3FdbG11nYbWBuv9nPbe5PEyayxgAMAsF1bCKTnIZIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIPQxxpgddDg+xlQAAIB0/bFXS+Nvvv36SjN52q72c9t7kwQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAQSQBAAAEkQQAABBEEgAAQBBJAAAAoY8xxuygw/ExpvLBeve4lMZfXtT6svfS8FXMX/nt2cJ5rFqW2oXY7c7gJBStcS+fw70EbIe1fRuq3w/n8N1w/YO/WBr/9j//sdL4rZzDq/3c9t4kAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEAQSQAAAEEkAQAABJEEAAAQRBIAAEDoY4wxO+hwfIypvH/L/JQf2PW+wkye3xrH0FvtGE58Cs7CCpex3S21newvahdyjWNwL9W5DrCeLTxP1Tl4ntdx6utwDr9Zr7//9fI+bn7t1RVmUnO1n9vemySAM+AHFazH8wTr2UIgPQ+RBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAABBJAEAAASRBAAAEEQSAABAEEkAAAChjzHG7KDD8TGmAjxVt8elNP5y7/+aqrtleil/4GLXV5gJVfPfyvd1lxHYkOtPfLm8j7f+1efL+3j5cm5x9MsEAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAIJIAgAACCIJAAAgiCQAAIAgkgAAAEIfY4zZQb97W/zQXhu/hmWZPux7evEg1jgH1WPY7WqTmL9z1reFe6mqeh1b8Rz06g5aa8+xjNyfQ/FCLivcjBfF5+HU1ngeq89TdQ7n8DxvgetwHrZwHbcwh6rqMYxW/K21hZNwYmt8R/+hH/0n5X28/fVXprb3JgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIIgkAACCIJAAAgCCSAAAAgkgCAAAIfYwxTj0JAACArfAmCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAgiCQAAIIgkAACAIJIAAACCSAIAAAj/ByOu98FHZQ+lAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix(dataloader, model, full_dataset.labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 89 %\n",
      "----------\n",
      "Top1 accuracy of the network: 90 %\n"
     ]
    }
   ],
   "source": [
    "accuracy(dataloader, model, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loading the Resnet18 with pretrained weights and freezing the models layers.\n",
    "\n",
    "if model_:\n",
    "    model = torchvision.models.resnet50(pretrained=False)\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(model_))\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    model = torchvision.models.resnet50(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Adding FC layer to train classifier. Required_grad is true by default here.\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "# Setting loss and optimizer. I chose Cross Entropy Loss because it is moustly used loss function when\n",
    "# training for multiclass classification. Adam is also just state of the art optimization algorithm.\n",
    "# It has its downsides but in my opinion still a better choice here than a SGD.\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n",
      "train Loss: 0.4043 Acc: 0.8810\n",
      "val Loss: 0.4356 Acc: 0.8718\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "model = fit(dataloader, model, criterion, optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './doodle.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
